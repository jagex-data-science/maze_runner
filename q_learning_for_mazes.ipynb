{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your own maze-solving AI agent in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning (RL) is a subfield of machine learning/AI in which an agent acts in an environment and learns to maximise the total reward it receives. RL has gained fame from video game AIs that have learned to play [Atari games](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/), autonomous [helicopters that have learned to do some wild aerial manuevres](https://www.youtube.com/watch?v=VCdxqn0fcnE), and humanoid robots that have learned how to best [adjust limb positions and joint torques to stand up](https://gym.openai.com/envs/HumanoidStandup-v1) (sort of..) and even [walk and manoeuvre complex obstacle courses](https://www.youtube.com/watch?v=gn4nRCC9TwQ) (with only moderate crazy arm flailing). \n",
    "\n",
    "In this post, we want to make a reinforcement learning agent that can learn to escape a grid-like environment as quickly as possible. Although it's probably relatively trivial to code up a solution to solve a given maze, the point of using RL is that it is extremely general - the Q learning approach explained here can be used to solve many diverse problems. So, although it might seem like we are using a very big hammer to smash quite a small fly, the idea is that once you understand RL in a digestible toy setting, you can begin to transfer the principles to more complex tasks. \n",
    "\n",
    "In our maze problem, there is a worm that needs to escape the maze as quickly as possible:\n",
    "\n",
    "<a href=\"url\"><img src=\"https://raw.githubusercontent.com/jagex-data-science/maze_runner/master/basic_grid.png\" align=\"center\" height=\"200\" alt='Basic maze.' ></a>\n",
    "\n",
    "This is a pretty tame maze (nothing really on it - just +100 reward for getting to the end, and -1 everywhere else), so let's also make an advanced maze surrounded with dragon-filled lava (-100 reward, not good), a couple of perfectly-spherical trolls (-20 reward) and a treat (+20 reward, maybe a sugar cube... what do worms eat again?).\n",
    "\n",
    "<a href=\"url\"><img src=\"https://raw.githubusercontent.com/jagex-data-science/maze_runner/master/advanced_grid.png\" align=\"center\" height=\"300\" alt='Advanced maze.' ></a>\n",
    "\n",
    "The point is that we can use RL to solve all sorts of problems, including all sorts of mazes. We'll use basic Python (+numpy/pandas) to allow the worm to learn how 'good' each state and action is - this goodness is represented by the values in the heatmap below. In this post, you'll see that the major trick is that these values incorporate information about potential future rewards leading on from that state. This means that once the agent knows these values (called Q values - we'll discuss), it can then make greedy decisions that maximise this value at each time step. This lets us solve simple grid worlds. We can then plot our trajectory within the grid world/maze, and even after just one training episode you can end up with a maze solution, which can be visualised as a GIF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Solving the maze.](https://raw.githubusercontent.com/jagex-data-science/maze_runner/master/q2_traversal_in_10_steps.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want the code to play around with, check out [this notebook](https://github.com/jagex-data-science/maze_runner/blob/master/q_learning_for_mazes_just_code.ipynb). The rest of the post will go through what the code is doing. In the next post, we'll make use of the code that is in place to do some experiments for seeing how changes to parameters (learning rate, exploration/exploitation trade-offs, number of episodes, episode length, on-policy/off-policy methods) affect our learned strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The maze problem\n",
    "\n",
    "First let's introduce some language for talking about RL problems, and then get into the code.\n",
    "\n",
    "RL deals with sequential environments indexed by time points. We say that at any time point the worm can be in any number of ***states*** (the different squares on the grid) and can take various ***actions*** from each state (here, our action space is `[left, right, up and down]` - but in some spots, like corner states or next to obstacles, not all actions are available). Each state is associated with a ***reward*** $r$ - this is just a number, and is a property of the environment. So, we can make decisions about which action $a$ to take in a given state $s$ at time point $t$, and the environment gives a reward $r$ and puts us in a ***successor state*** $s'$. Each episode ends when the worm gets to ***terminal state*** - the end of the maze. Then, the worm is (cruelly) transported back to the beginning where it has to start again. Let's say there's no dying/respawning, so if you get into the lava you're punished severely (-100 reward) but you can still crawl out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "Let's start setting up the environment. We need a reward matrix. For the simple maze, the numpy reward matrix looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports + a bit of setup first\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "sns.set(font=\"monospace\") # hipster font\n",
    "random.seed(42)\n",
    "\n",
    "# basic maze state rewards\n",
    "reward_matrix = np.array([[-1, -1, -1, -1, -1, -1],\n",
    "                          [-1, -1, -1, -1, -1, -1],\n",
    "                          [-1, -1, -1, -1, -1, -1],\n",
    "                          [-1, -1, -1, -1, -1, -1],\n",
    "                          [-1, -1, -1, -1, -1, -1],\n",
    "                          [-1, -1, -1, -1, -1, 100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the complicated maze, we've added a few other rewards, a lava moat, and a few obstacles (represented as rewards of 0). The numpy version is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-100., -100., -100., -100., -100., -100., -100., -100.],\n",
       "       [-100.,   -1.,    0.,   -1.,   -1.,   -1.,   -1., -100.],\n",
       "       [-100.,   -1.,  -20.,   -1.,   20.,   -1.,   -1., -100.],\n",
       "       [-100.,   -1.,   -1.,   -1.,   -1.,   -1.,   -1., -100.],\n",
       "       [-100.,   -1.,    0.,   -1.,   -1.,  -20.,   -1., -100.],\n",
       "       [-100.,   -1.,   -1.,   -1.,    0.,   -1.,   -1., -100.],\n",
       "       [-100.,   -1.,   -1.,   -1.,    0.,   -1.,  100., -100.],\n",
       "       [-100., -100., -100., -100., -100., -100., -100., -100.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# advanced maze state rewards\n",
    "reward_matrix = np.array([[-1,   0, -1,  -1,  -1,  -1],\n",
    "                          [-1, -20, -1,  20,  -1,  -1],\n",
    "                          [-1,  -1, -1,  -1,  -1,  -1],\n",
    "                          [-1,   0, -1,  -1, -20,  -1],\n",
    "                          [-1,  -1, -1,   0,  -1,  -1],\n",
    "                          [-1,  -1, -1,   0,  -1, 100]]).astype(\"float32\")\n",
    "\n",
    "reward_matrix = np.pad(reward_matrix, pad_width=1, mode='constant', constant_values=-100)\n",
    "reward_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just stick to this more interesting matrix for now. Remember that this reward matrix corresponds to this maze situation:\n",
    "\n",
    "<img src=https://raw.githubusercontent.com/jagex-data-science/maze_runner/master/advanced_grid.png alt='Advanced maze.' style=\"float: left; width: 600px;\" \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The worm can go through the maze using four actions - up, down, left or right: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actions = np.array(['up', 'down', 'left', 'right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's easy to see how these actions change your position in grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move(current_state, action):\n",
    "    \n",
    "    next_state_x, next_state_y = current_state\n",
    "    \n",
    "    if action=='up':\n",
    "        next_state_x -=1\n",
    "    elif action=='down':\n",
    "        next_state_x +=1\n",
    "    elif action=='left':\n",
    "        next_state_y -=1\n",
    "    elif action=='right':\n",
    "        next_state_y +=1\n",
    "\n",
    "    return [next_state_x, next_state_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, with numpy matrix indexing the upper left corner is [0, 0]. If we go right, this takes us to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move([0,0], 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we're in the bottom right corner [7, 7] and we go up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move([7,7], 'up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a little function to get the possible actions based on current position - usually we can go any of the 4 directions, unless we're at the edge of the grid or next to an obstacle (obstacles are where the reward matrix has a value of 0, remember):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_possible_actions(current_state, possible_actions=actions, reward_matrix=reward_matrix):\n",
    "\n",
    "    nrows, ncols = reward_matrix.shape\n",
    "    x, y = current_state\n",
    "    \n",
    "    # checking first for grid corners\n",
    "    # then for obstacles (where reward==0)\n",
    "    if y==ncols-1 or reward_matrix[x, y+1]==0:\n",
    "        possible_actions = np.setdiff1d(possible_actions, 'right')\n",
    "    if y==0 or reward_matrix[x, y-1]==0:\n",
    "        possible_actions = np.setdiff1d(possible_actions, 'left')\n",
    "    if x==0 or reward_matrix[x-1, y]==0:\n",
    "        possible_actions = np.setdiff1d(possible_actions, 'up')\n",
    "    if x==nrows-1 or reward_matrix[x+1, y]==0:\n",
    "        possible_actions = np.setdiff1d(possible_actions, 'down')\n",
    "        \n",
    "    return possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the possible actions in the lava corner [0,0] (take a look at the original grid) are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['down', 'right'], \n",
       "      dtype='|S5')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_possible_actions([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the possible actions reflect the presence of obstacles, like in the position [4,3] where we can't go left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['down', 'right', 'up'], \n",
       "      dtype='|S5')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_possible_actions([4,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sometimes we have both edge cases and obstacles to restrict our movement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['left', 'right'], \n",
       "      dtype='|S5')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_possible_actions([7,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have figured out how the worm can move on the grid. The interesting part is learning how good it is for the worm to be in any state and take some action, so let's start diving into this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some of the maths behind the problem\n",
    "\n",
    "This section isn't really crucial to the post so feel free to skip it, but I recently learned Latex so I've got an itch to type equations. \n",
    "\n",
    "As the worm goes through the maze, it will acquire a string of rewards. It will get some reward for time step $t$, $t+1$, $t+2$ and so on. The total reward in an episode (the **return $G_t$** from time point $t$ onwards) is just the sum:\n",
    "\n",
    "\\begin{align*}\n",
    "    G_t = R_{t+1} + R_{t+2} + R_{t+3} \\ ... \\ + R_{n} = \\sum_{t}^n R_{t+1}\n",
    "\\end{align*}\n",
    "\n",
    "We want to encode some preference for immediate rewards, with future rewards being worth less because they are more uncertain and we generally have a preference towards recency. We can do this with **gamma discounting** - the parameter $\\gamma$ controls our preference for immediate versus long-term rewards. \n",
    "\n",
    "\\begin{align*}\n",
    "    G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\ ... \\ = \\sum_{k=0}^{\\inf} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "\n",
    "The gamma parameter can be any value between 0 and 1. At the extremes, if $\\gamma$==0, then we are only interested in the most immediate rewards (all rewards after time $t$ are multiplied by 0 and disappear); if $\\gamma$==1, then we are equally as interested in far away future rewards we are in the most immediate reward. So, this introduces a judgement where we prefer immediate reward (unless $\\gamma$ is 1). If we set our discount factor to be $\\gamma$=0.5, this would mean that we halve the amount we care about returns at each step. Notice that the very next reward $R_{t+1}$ isn't discounted by $\\gamma$.\n",
    "\n",
    "The **value of a state** $v(s)$ is actually defined as the expectation of the return when you start in that state: \n",
    "\n",
    "\\begin{align*}\n",
    "    v(s) = \\mathbb{E}[G_t \\ | \\ S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "So, how good a particular state is depends on how much reward you expect to get in the future leading on from the state. Let's substitute the definition of $G_t$ back in:\n",
    "\n",
    "\\begin{align*}\n",
    "    v(s) =& \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 R_{t+3} + ... \\ | \\ S_t = s]\n",
    "\\end{align*}\n",
    "   \n",
    "    \n",
    "You might have noticed that the equation for the return $G_t$ has a bit of a recursive structure. It's made up of two parts - the immediate reward $R_{t+1}$ and the discounted rewards for $t+2$ onwards (i.e. $\\gamma R_{t+2} + \\gamma ^2 R_{t+3} + ... $). But, this second part is just the $\\gamma$ times the rewards for the next time step onwards!\n",
    "\n",
    "\\begin{align*}\n",
    "    v(s) =& \\ \\mathbb{E} [G_t \\ | \\ S_t = s] \\\\\n",
    "         =& \\  \\mathbb{E} [R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 R_{t+3} + ... \\ | \\ S_t = s] \\\\\n",
    "         =& \\ \\mathbb{E} [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + ... ) \\ | \\ S_t = s] \\\\\n",
    "         =& \\ \\mathbb{E} [R_{t+1} + \\gamma (G_{t+1}) \\ | \\ S_t = s] \n",
    "\\end{align*}\n",
    "\n",
    "In the expanded definition of $G_t$, if we factorise out the $\\gamma$ discounting factor, you can see that $\\gamma (R_{t+2} + \\gamma R_{t+3} + ... $ is simply $\\gamma$ times the value function of the next state $t+1$ ($G_{t+1}$). \n",
    "In other words, the value function is recursively defined (this is a version of the [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation)). \n",
    "\n",
    "tl;dr The point of this section is to emphasise that the value of a state depends on the rewards you expect to see in future states after it. We'll use this idea to solve the maze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should the worm solve the maze?\n",
    "\n",
    "Intuitively, for each state we find ourselves in, we want to choose the action that maximises future cumulative reward. If at every point, we knew what the best possible action was (the one that is going to maximise future cumulative reward), then we've solved the problem. We would just go ahead and take the best action at all times. Because the biggest reward is at the end of the maze, these 'best actions' should gradually take us closer to our ultimate goal of escape.\n",
    "\n",
    "This means that the problem the worm actually has to solve is estimating how good it is to take different actions in each state. It has no prior knowledge or model of the environment - to learn these estimates, it'll just have to explore around and gradually build up information.\n",
    "\n",
    "Formally, these estimates are ***utility*** values or **Q values**. For each state $s$ and action $a$ available in that state, there is a corresponding $Q(s,a)$ we want to learn. These utility values tell us how good an action is in a given state. It's defined as the immediate reward you get for making an action (this is the $r(s,a)$ part), plus the best/max utility you can get from the next state, the successor state $s'$:\n",
    "\n",
    "\\begin{align*}\n",
    "    Q(s,a) =& \\ r(s,a) + \\gamma \\max_{a'} Q(s', a')\n",
    "\\end{align*}\n",
    "\n",
    "Initially, when we enter the maze for the first time, we have no idea what these Q values are. We could initialise all values with 0 or with some random small number (e.g. between 0 and 0.1). This is a bit abstract... looking at a concrete example helps. The below function will generate a starting Q table (pass `use_zero=True` to initialise with zeros if you prefer... but change the obstacle labelling too):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialise_q_table(reward_matrix, obstacle_list = ['[1, 2]', '[4, 2]', '[5, 4]', '[6, 4]'], use_zero=False):\n",
    "    \n",
    "    nrows, ncols = reward_matrix.shape\n",
    "    \n",
    "    # get possible states and actions\n",
    "    states = pd.DataFrame([[[x, y]] for x in range(nrows) for y in range(ncols)], columns=['state'])\n",
    "    states['str_state'] = [str(state) for state in states['state']]\n",
    "    states['actions'] = [get_possible_actions(state) for state in states['state']]\n",
    "\n",
    "    # clean up actions, one row per possible action\n",
    "    clean_actions = pd.DataFrame(states['actions'].apply(pd.Series, 1).stack(), columns=['action'])\n",
    "    clean_actions.index = clean_actions.index.droplevel(-1)\n",
    "    states = states.join([clean_actions]).drop('actions', axis=1)\n",
    "\n",
    "    # initiate values to be 0 or random small numbers\n",
    "    if use_zero:\n",
    "        states['value'] = 0\n",
    "    else:\n",
    "        states['value'] = np.random.uniform(-0.1,0.1,states.shape[0])\n",
    "        states.loc[states['str_state'].isin(obstacle_list), 'value'] = 0\n",
    "        \n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to see what this table looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>str_state</th>\n",
       "      <th>action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.031696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.039593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.071186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.079135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.020249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.059783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.044383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.015165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.023071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.081889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.069563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.086690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.027347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0, 5]</td>\n",
       "      <td>[0, 5]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.039143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0, 5]</td>\n",
       "      <td>[0, 5]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.073352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0, 5]</td>\n",
       "      <td>[0, 5]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.083652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0, 6]</td>\n",
       "      <td>[0, 6]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.054499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0, 6]</td>\n",
       "      <td>[0, 6]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.076243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0, 6]</td>\n",
       "      <td>[0, 6]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.097568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0, 7]</td>\n",
       "      <td>[0, 7]</td>\n",
       "      <td>down</td>\n",
       "      <td>-0.054722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state str_state action     value\n",
       "0  [0, 0]    [0, 0]   down  0.031696\n",
       "0  [0, 0]    [0, 0]  right -0.039593\n",
       "1  [0, 1]    [0, 1]   down  0.071186\n",
       "1  [0, 1]    [0, 1]   left -0.079135\n",
       "1  [0, 1]    [0, 1]  right  0.020249\n",
       "2  [0, 2]    [0, 2]   left  0.059783\n",
       "2  [0, 2]    [0, 2]  right -0.044383\n",
       "3  [0, 3]    [0, 3]   down  0.015165\n",
       "3  [0, 3]    [0, 3]   left  0.023071\n",
       "3  [0, 3]    [0, 3]  right -0.081889\n",
       "4  [0, 4]    [0, 4]   down  0.069563\n",
       "4  [0, 4]    [0, 4]   left -0.086690\n",
       "4  [0, 4]    [0, 4]  right  0.027347\n",
       "5  [0, 5]    [0, 5]   down  0.039143\n",
       "5  [0, 5]    [0, 5]   left  0.073352\n",
       "5  [0, 5]    [0, 5]  right -0.083652\n",
       "6  [0, 6]    [0, 6]   down  0.054499\n",
       "6  [0, 6]    [0, 6]   left  0.076243\n",
       "6  [0, 6]    [0, 6]  right -0.097568\n",
       "7  [0, 7]    [0, 7]   down -0.054722"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = initialise_q_table(reward_matrix, use_zero=False)\n",
    "q.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are storing our estimates of $Q(s,a)$ in a table, with one value for each state we can be in and each action we can take in that state.\n",
    "\n",
    "For example, the current (randomly generated) Q value associated with starting in position [0,0] on the grid (in lava, actually) and taking action 'down' is 0.056920, and going right is -0.053291. These are random numbers, but if these were actually learned, then this would be useful information when deciding whether to go down or right from [0,0] - probably go down. \n",
    "\n",
    "Just to recap where we are so far, let's draw our reward grid and this gibberish grid of randomised Q values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_q_values(q, reward_matrix, plot_reward=False, save_q_plot=True, t=0, episode=0):\n",
    "    \n",
    "    nrows, ncols = reward_matrix.shape\n",
    "    avg_q_value_by_state = q.groupby('str_state')['value'].mean().values.reshape(nrows,ncols)\n",
    "\n",
    "    if plot_reward:\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "        sns.heatmap(reward_matrix, ax=ax1)\n",
    "        sns.heatmap(avg_q_value_by_state, ax=ax2)\n",
    "        fig.set_size_inches(16, 6)\n",
    "    else: \n",
    "        plt.figure(figsize=(7,6))\n",
    "        heatmap = sns.heatmap(avg_q_value_by_state) \n",
    "        if save_q_plot:\n",
    "            heatmap.get_figure().savefig('/Users/natasha_latysheva/Projects/jagex_data_science_blog/avg_q_value_episode_' \\\n",
    "                                         + str(episode) + '_time_' + str(t) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5QAAAFlCAYAAACUZ6fdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncPmTweISCwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1111a3d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_q_values(q, reward_matrix, plot_reward=True, save_q_plot=True, t=0, episode=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left hand plot is the reward grid, which looks like what we'd expect (check out the initial figure of the maze at the top of this post). And yup, the right hand plot (Q values) looks pretty random. A little bit [Mondrian](https://www.google.co.uk/search?q=mondrian+paintings&source=lnms&tbm=isch&sa=X&ved=0ahUKEwjP2dPs4NvUAhWDPBQKHXSSAM4Q_AUICigB&biw=2100&bih=1127)?\n",
    "\n",
    "How do we move beyond random Q values? How do we learn useful Q values that can help us navigate a maze? For that, we'll need to do some ***Q learning*** (of [DeepMind Atari fame](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)). There's actually many different varieties of Q-learning, and here we have a special simple case where there are very few, discrete states and actions (this is a tabular method), we aren't doing any feature extraction from the environment (e.g. no [deep convnets for Q learning](https://vmayoral.github.io/robots,/ai,/deep/learning,/rl,/reinforcement/learning/2016/08/07/deep-convolutional-q-learning/)), and we aren't keeping any sort of running memory (e.g. [eligibility traces](https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q function update rule\n",
    "\n",
    "We will learn the real Q values iteratively using a simple update rule. Basically, to update $Q(s, a)$ at time $t$ (so, $Q(s_t, a_t)$), we will do a one step look ahead and find the maximum Q value you could immediately get from that successor state. In other words we peek ahead one step and find the estimate of the optimal future state. This is $\\max_{a} Q(s_{t+1}, a_t)$. \n",
    "\n",
    "More precisely, we take our previous Q value $Q(s_t, a_t)$ for a state-action pair at time t, and make the following update:\n",
    "\n",
    "\\begin{align*}\n",
    "    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha*\\big(r_{t+1} + \\gamma*  max_a Q(s_{t+1}, a_t) -  Q(s_t, a_t)\\big)   \n",
    "\\end{align*}\n",
    "\n",
    "The alpha parameter $\\alpha$ controls your learning rate (higher values weight new information higher, which overwrites old information more). As mentioned before, the gamma parameter $\\gamma$ (the 'discount factor') controls how much we prefer recent rewards to far-away rewards - if it's 0, we only ever care about immediate rewards, and if it's 1, we care about very distant rewards exactly as much as recent ones. You can change up alpha and gamma in clever ways as the algorithm trains but we'll just set them to... 0.8. :)\n",
    "\n",
    "It's interesting to note that if you set alpha to 1.0 then the update rule becomes very short:\n",
    "\n",
    "\\begin{align*}\n",
    "    Q(s_t, a_t) = r_{t+1} + \\gamma*  max_a Q(s_{t+1}, a_t)    \n",
    "\\end{align*}\n",
    "\n",
    "This is a recursive definition, since the current Q value depends on the immediate reward plus the discounted maximum future Q value. In this way, the $Q(s,a)$ estimates take into account the Q values of future states. The values will gradually converge after many iterations (often surprisingly few iterations), as information about future rewards eventually trickles back to the value estimates of states at the beginning of the maze. This will become clear through the code examples.\n",
    "\n",
    "The code for doing just one Q update is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_q_value(q, current_state, next_state, action, alpha, gamma, terminal_state, verbose=False):\n",
    "    \n",
    "    # make move and collect immediate reward\n",
    "    reward = reward_matrix[next_state[0], next_state[1]]\n",
    "    \n",
    "    # the current q for the state + action we just took\n",
    "    current_q = q.loc[((q['str_state']==str(current_state)) & (q['action']==action)), 'value'].values[0]\n",
    "        \n",
    "    # get maximum q value leading on from next_state\n",
    "    max_future_q_value = np.max(q[q['str_state']==str(next_state)]['value'])\n",
    "    \n",
    "    # q value update rule\n",
    "    new_q = current_q + alpha*(reward + gamma*max_future_q_value - current_q)\n",
    "     \n",
    "    # update relevant value in q table\n",
    "    q.loc[((q['str_state']==str(current_state)) & (q['action']==action)), 'value'] = new_q\n",
    "    \n",
    "    # if next state is the terminal state, update that q value to reward\n",
    "    if np.array_equal(next_state, terminal_state):\n",
    "        q.loc[(q['str_state']==str(next_state)), 'value'] = 100\n",
    "\n",
    "    if verbose:\n",
    "        print 'Q of state {0} taking action \"{1}\" was updated from {2} to {3}'.format(current_state, action, current_q, new_q)\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to do a one step update. Remember this was our randomly-generated q table so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>str_state</th>\n",
       "      <th>action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.031696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.039593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.071186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.079135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.020249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.059783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.044383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.015165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.023071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.081889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state str_state action     value\n",
       "0  [0, 0]    [0, 0]   down  0.031696\n",
       "0  [0, 0]    [0, 0]  right -0.039593\n",
       "1  [0, 1]    [0, 1]   down  0.071186\n",
       "1  [0, 1]    [0, 1]   left -0.079135\n",
       "1  [0, 1]    [0, 1]  right  0.020249\n",
       "2  [0, 2]    [0, 2]   left  0.059783\n",
       "2  [0, 2]    [0, 2]  right -0.044383\n",
       "3  [0, 3]    [0, 3]   down  0.015165\n",
       "3  [0, 3]    [0, 3]   left  0.023071\n",
       "3  [0, 3]    [0, 3]  right -0.081889"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we start in [0,0] and take one step to the right, to cell [0,1]. How would the `Q(s=[0,0], a='right')` be be updated from -0.053291 after this one move? This move actually puts us into a lava cell on the grid, so we hope that the negativity of this would be reflected in the updated value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q of state [0, 0] taking action \"right\" was updated from -0.0395929640607 to -79.9623592528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-100.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_q_value(q, current_state=[0,0], next_state=[0,1], action='right', \n",
    "               alpha=0.8, gamma=0.8, terminal_state=[6,6], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new value really negative value replaces the previous `Q(s=[0,0], a='right')` entry in the table. \n",
    "\n",
    "We can double check this by hand. Recall that the reward is -100 for being in lava, `alpha` and `gamma` are both 0.8, and the previous estimate of $Q(s,a)$ was -0.0532913297037. \n",
    "\n",
    "The only tricky part is that we need to find the highest Q value associated with the successor state. Our starting state was [0,0] and the action we chose (maybe randomly) was 'right', so this means that the successor state is [0,1]. Looks like there are three actions we can take at state [0,1]: down, left, right. Of these, going down has the highest Q value (namely, 0.056772). Plugging these values in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-79.97432418594074"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_q = -0.0532913297037 + 0.8*(-100 + 0.8*0.056772 - -0.0532913297037)\n",
    "new_q   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! What if we then went back left, from [0,1] to [0,0]? Q table looks like this now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>str_state</th>\n",
       "      <th>action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.031696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>right</td>\n",
       "      <td>-79.962359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.071186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.079135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.020249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.059783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.044383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.015165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.023071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.081889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state str_state action      value\n",
       "0  [0, 0]    [0, 0]   down   0.031696\n",
       "0  [0, 0]    [0, 0]  right -79.962359\n",
       "1  [0, 1]    [0, 1]   down   0.071186\n",
       "1  [0, 1]    [0, 1]   left  -0.079135\n",
       "1  [0, 1]    [0, 1]  right   0.020249\n",
       "2  [0, 2]    [0, 2]   left   0.059783\n",
       "2  [0, 2]    [0, 2]  right  -0.044383\n",
       "3  [0, 3]    [0, 3]   down   0.015165\n",
       "3  [0, 3]    [0, 3]   left   0.023071\n",
       "3  [0, 3]    [0, 3]  right  -0.081889"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd expect this change (see if these values make sense to you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-79.97859220000001"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_q = -0.075105 + 0.8*(-100 + 0.8*0.056920 - -0.075105)\n",
    "new_q   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which our function should be able to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q of state [0, 1] taking action \"left\" was updated from -0.0791350444916 to -79.9955416022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-100.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_q_value(q, current_state=[0,1], next_state=[0,0], action='left', \n",
    "               alpha=0.8, gamma=0.8, terminal_state=[6,6], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so these updates are reflecting the fact that being in lava is bad. What would happen to the Q value next to the terminal state, the escape corner at [6,6]? Say we start in state [6, 5] and take a right. Let's pull out the relevant rows of Q values from the table (note: the `str_state` column is just for pandas dataframe subsetting convenience):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>str_state</th>\n",
       "      <th>action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>down</td>\n",
       "      <td>-0.096994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.023344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>up</td>\n",
       "      <td>-0.044521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state str_state action     value\n",
       "53  [6, 5]    [6, 5]   down -0.096994\n",
       "53  [6, 5]    [6, 5]  right  0.023344\n",
       "53  [6, 5]    [6, 5]     up -0.044521"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[q['str_state']=='[6, 5]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q of state [6, 5] taking action \"right\" was updated from 0.0233437529197 to 80.0677103509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_q_value(q, current_state=[6,5], next_state=[6,6], action='right', \n",
    "               alpha=0.8, gamma=0.8, terminal_state=[6,6], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The fact that there is a big reward at the end of taking the action 'right' from state [6,5] gets reflected in a higher `Q(s=[6,5], a='right')` value. The rewards leading on from a state get reflected in the Q values of that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning all the Q values iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above little tidbits showed the effect of single updates in isolation, but of course we want to do many, many updates during our training episodes, until the point that the Q value estimates are stable. Once stability/convergence is reached, the hope is that the new table will accurately represent how good it is to be in any state and take any action in our environment. \n",
    "\n",
    "Again, the point of going through all this pain is that these techniques are super general - if you have an environment where you can make actions that are associated with reward, some variant of Q learning is probably applicable. \n",
    "\n",
    "Just one more thing to think about before we run the training. It's actually possible to learn good Q values by randomly walking around the maze (taking actions completely at random) and just observing which rewards come your way. Even though we'd be building better and better estimates of how to optimally behave, we aren't following that advice, and instead are using a totally different, random policy to move around the world. But another approach is to take actions that you think are good, at the same time as you're building your estimates of how good they are. This would mean that your actions would be influenced by the Q values you learn in real time as you learn about them.\n",
    "\n",
    "So basically, we can learn about the optimal policy while being maximally exploratory. Alternatively, as we gather information, we can let this immediately change how we behave. \n",
    "\n",
    "The parameter epsilon ($\\epsilon$) controls to what extent our decisions about actions will be random. Do we *always* want to make random decisions about movement (set $\\epsilon$>1), or only *sometimes* make random decisions (0<$\\epsilon$<1), or *always* go where the utility values look best ($\\epsilon$=0)? Let's try setting `epsilon=0.5`, which will lead to random movements 50% of the time (quite high, actually), but 50% of the time we choose the best-looking action (to the best of our knowledge at that point).\n",
    "\n",
    "The function to run the entire simulation is this (explanation below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_maze(reward_matrix, lava_grid=True, alpha=0.8, gamma=0.8, epsilon=0.5, \n",
    "               n_episodes=15, t_per_episode=1000, verbose=False, \n",
    "               plot_often=False, plot_episode_end=True):\n",
    "\n",
    "    # initialise q table\n",
    "    q = initialise_q_table(reward_matrix, use_zero=False)\n",
    "\n",
    "    # episodes end when terminal state reached (escaped==True)\n",
    "    for episode in range(int(n_episodes)):\n",
    "\n",
    "        print 'Commencing episode number {0}...'.format(episode)\n",
    "        t = 0\n",
    "        escaped = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        # arbitrarily begin in the top left non-lava corner of the grid\n",
    "        if lava_grid:\n",
    "            current_state = [1,1]\n",
    "            terminal_state = [6,6]  \n",
    "        # if using simple maze\n",
    "        else:\n",
    "            current_state = [0,0]\n",
    "            terminal_state = [5,5]\n",
    "\n",
    "        # while you're in the maze, make decisions about actions\n",
    "        while not escaped and t < t_per_episode:\n",
    "            \n",
    "            # get q table info relating to your current state\n",
    "            current_state_slice = q[q['str_state'] == str(current_state)]\n",
    "\n",
    "            # small chance of acting randomly (epsilon)\n",
    "            if np.random.rand() < epsilon:\n",
    "                valid_moves = np.array(current_state_slice['action'])\n",
    "                action = np.random.choice(valid_moves)\n",
    "\n",
    "            # otherwise, do what looks best\n",
    "            else:\n",
    "                'Taking best action'\n",
    "                best_action_index = np.argmax(np.array(current_state_slice['value']))\n",
    "                action = np.array(current_state_slice['action'])[best_action_index]           \n",
    "\n",
    "            next_state = move(current_state, action)\n",
    "\n",
    "            # get your reward and update your q value\n",
    "            total_reward += update_q_value(q, current_state, next_state, action, alpha=alpha, gamma=gamma,\n",
    "                                           terminal_state=terminal_state, verbose=verbose)\n",
    "\n",
    "            # if you reached the terminal state, you win gg\n",
    "            if np.array_equal(next_state, terminal_state):\n",
    "                print 'Escaped maze in {0} time steps with {1} total reward.'.format(t, total_reward)\n",
    "                escaped = True\n",
    "                \n",
    "                if plot_episode_end:\n",
    "                    plot_q_values(q, reward_matrix, save_q_plot=True, t=t, episode=episode)\n",
    "\n",
    "            # update current state and time step\n",
    "            # occasionally plot q values\n",
    "            if plot_often:\n",
    "                if t == 0 or t % 500 == 0:\n",
    "                    print 'Current state at time {0} is: {1}'.format(t, current_state)\n",
    "                    plot_q_values(q, reward_matrix, save_q_plot=True, t=t, episode=episode)\n",
    "\n",
    "            t += 1\n",
    "            current_state = next_state\n",
    "            \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is a bit dense-looking but is actually composed of very simple things:\n",
    "\n",
    "+ **Line 6**: Initialise the Q values randomly (or to all zeros)\n",
    "+ **Line 9**: We'll let the worm do many different runs (episodes) through the maze \n",
    "+ **Lines 17-23**: Depending on whether you want to run the basic maze or the advanced maze with the lava moat, the starting position and the position (the terminal state) will vary. \n",
    "+ **Line 26**: Given that you haven't escaped (reached the terminal state), and your time hasn't run out (arbitrary n), make decisions about where to go next in the grid.\n",
    "+ **Line 29**: Just taking a slice of the Q table that relates to our current state, for convenience, since we'll be using these values below.\n",
    "+ **Lines 32 to 33**: Sample a random value between 0 and 1 using np.random.rand(). If this is less than epsilon, choose a random action out of the available ones\n",
    "+ **Lines 37 to 40**: If the random value wasn't less than epsilon, choose the action associated with the highest utility. \n",
    "+ **Line 42**: whichever action you chose, now's the time to get the next state it leads to.\n",
    "+ **Line 45**: Update your Q value by looking ahead one step from this `next_state` (see previous sections for explanation) and keep a tally of your running reward.\n",
    "+ **Lines 49 to 54**: Check if this next state is actually the terminal state. If it is, you reached the end of the maze! Make a self-congratulatory plot of the q table as it stands (if `plot_episode_end=True`) and set `escaped` to `True`.\n",
    "+ **Lines 58 to 51**: A bit of monitoring code - if you'd like to keep a closer tab on the Q values as they evolve, pass `plot_often=True`. \n",
    "+ **Lines 63 and 64**: Incrementing the time step and updating our current state.\n",
    "+ **Line 66**: This function returns the updated q table, our estimate of the utility of state-action pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run just ONE episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing episode number 0...\n",
      "Escaped maze in 389 time steps with -14572.0 total reward.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAFlCAYAAADMCx4cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111e1a810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = learn_maze(reward_matrix, lava_grid=True, alpha=0.8, gamma=0.8, epsilon=0.9, \n",
    "               n_episodes=1, t_per_episode=10000, verbose=False, plot_often=False, plot_episode_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see that reward grid again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4gAAAFlCAYAAACp9ca7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111bb06d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_q_values(q, reward_matrix, plot_reward=True, save_q_plot=True, t=0, episode=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, our learned Q values definitely look inspired by the reward grid, even after just 1 short episode with not that many time steps. Given that we were plonked into this world with absolutely zero knowledge about the world (we could only try things out and get feedback from the environment), we now already know quite a bit. But because we've only done one episode, the information from the highest reward state (the end) hasn't really transferred back through the maze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the heatmap shows averaged Q values per state (i.e. averaged over available actions for each state, for visualisation purposes), it's hard to see some detail. Say in position [2,3], which is in between a -20 and +20 bonus, what are the Q values like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>str_state</th>\n",
       "      <th>action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>up</td>\n",
       "      <td>5.086925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>down</td>\n",
       "      <td>16.823792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>left</td>\n",
       "      <td>5.399838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>right</td>\n",
       "      <td>46.091993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state str_state action      value\n",
       "19  [2, 3]    [2, 3]     up   5.086925\n",
       "19  [2, 3]    [2, 3]   down  16.823792\n",
       "19  [2, 3]    [2, 3]   left   5.399838\n",
       "19  [2, 3]    [2, 3]  right  46.091993"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[q['str_state']=='[2, 3]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok whew, it's learned that going right is best and going left is worst. As we would expect.\n",
    "\n",
    "What about position [6,5], just one step away from the ultimate goal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>str_state</th>\n",
       "      <th>action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>down</td>\n",
       "      <td>-79.951800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.055869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>up</td>\n",
       "      <td>-0.001977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state str_state action      value\n",
       "53  [6, 5]    [6, 5]   down -79.951800\n",
       "53  [6, 5]    [6, 5]  right   0.055869\n",
       "53  [6, 5]    [6, 5]     up  -0.001977"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[q['str_state']=='[6, 5]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, going right is better than the other options.\n",
    "\n",
    "Go ahead and see how changing the number of episodes, episode length, and other parameters changes what you learn. For instance, below we are learning over 10 episodes, and we can see that after each run we seem to be escaping the maze in a roughly shorter amount of time and getting more reward. We are also building a more nuanced understanding of the Q values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing episode number 0...\n",
      "Escaped maze in 205 time steps with -4958.0 total reward.\n",
      "Commencing episode number 1...\n",
      "Escaped maze in 2071 time steps with -5514.0 total reward.\n",
      "Commencing episode number 2...\n",
      "Escaped maze in 263 time steps with -1945.0 total reward.\n",
      "Commencing episode number 3...\n",
      "Escaped maze in 35 time steps with 153.0 total reward.\n",
      "Commencing episode number 4...\n",
      "Escaped maze in 17 time steps with -252.0 total reward.\n",
      "Commencing episode number 5...\n",
      "Escaped maze in 363 time steps with -186.0 total reward.\n",
      "Commencing episode number 6...\n",
      "Escaped maze in 31 time steps with -283.0 total reward.\n",
      "Commencing episode number 7...\n",
      "Escaped maze in 83 time steps with -159.0 total reward.\n",
      "Commencing episode number 8...\n",
      "Escaped maze in 313 time steps with -1451.0 total reward.\n",
      "Commencing episode number 9...\n",
      "Escaped maze in 21 time steps with -75.0 total reward.\n"
     ]
    }
   ],
   "source": [
    "q2 = learn_maze(reward_matrix, lava_grid=True, alpha=0.8, gamma=0.8, epsilon=0.5, \n",
    "               n_episodes=10, t_per_episode=10000, verbose=False, plot_often=False, plot_episode_end=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4gAAAFlCAYAAACp9ca7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111b830d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_q_values(q2, reward_matrix, plot_reward=True, save_q_plot=True, t=0, episode=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10 episodes, it looks like the agent has learned enough about the environment to have a go at traversing it efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Q values to quickly leave the maze\n",
    "\n",
    "Now that we've gathered lots of information about the Q values (though normally we'd train for much, much longer), traversing the maze again is ez mode. For each state we're in, we are just going to greedily choose the available action with the highest utility, and keep doing that until we escape. \n",
    "\n",
    "The code for traversing the grid is simple (extremely simlar to `learn_maze` code above) but plotting is a bit of a pain. We will need two helper functions to make nice GIFs to visualise our path in the maze. The `plot_current_position` function takes your current position and plots it on the learned Q values heatmap. The `make_traversal_gif` function uses [imageio](https://imageio.readthedocs.io/en/latest/) magic to spit out a gif (make sure the `image_path` directory only has .png files that you want included in your GIF... clean out those cat memes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import glob\n",
    "import imageio\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traverse_grid(q, lava_grid=True, plot_each_step=True):\n",
    "    \n",
    "    print 'Traversing maze...'\n",
    "    \n",
    "    if lava_grid:\n",
    "        current_state = [1, 1]\n",
    "        terminal_state = [6, 6]\n",
    "    else:\n",
    "        current_state = [0, 0]\n",
    "        terminal_state = [5, 5]\n",
    "    \n",
    "    t=0\n",
    "    \n",
    "    # traverse greedily\n",
    "    while not np.array_equal(current_state, terminal_state):\n",
    "\n",
    "        print 'Current state is: {0}'.format(current_state)\n",
    "        \n",
    "        if plot_each_step:\n",
    "            plot_current_position(current_state, t, q)\n",
    "\n",
    "        # lookup q values of actions in current states\n",
    "        current_state_slice = q[q['str_state'] == str(current_state)]\n",
    "        best_action_index = np.argmax(np.array(current_state_slice['value']))\n",
    "        action = np.array(current_state_slice['action'])[best_action_index] \n",
    "        \n",
    "        # make a move\n",
    "        next_state = move(current_state, action)\n",
    "\n",
    "        # if you won then plot it\n",
    "        if next_state == terminal_state and plot_each_step==True:\n",
    "            plot_current_position(next_state, t+1, q, reached_terminal=True)\n",
    "            \n",
    "        current_state = next_state\n",
    "        t += 1\n",
    "    \n",
    "    # when done, make gif of path taken\n",
    "    make_traversal_gif(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_current_position(current_state, t, q, reward_matrix=reward_matrix, reached_terminal=False):\n",
    "    \n",
    "    # plot heatmap of q values\n",
    "    nrows, ncols = reward_matrix.shape\n",
    "    avg_q_value_by_state = q.groupby('str_state')['value'].mean().values.reshape(nrows,ncols)\n",
    "    plt.figure(figsize=(7,6))\n",
    "    heatmap = sns.heatmap(avg_q_value_by_state)        \n",
    "    \n",
    "    # add rectangle for current position\n",
    "    if not reached_terminal:\n",
    "        heatmap.add_patch(patches.Rectangle((current_state[1], 7-current_state[0]), 1, 1, \n",
    "                                       fill=True, alpha=0.25, linewidth=1))\n",
    "        heatmap.add_patch(patches.Rectangle((current_state[1], 7-current_state[0]), 1, 1, \n",
    "                                       fill=False, color='black', linewidth=5))\n",
    "    else: \n",
    "        heatmap.add_patch(patches.Rectangle((current_state[1], 7-current_state[0]), 1, 1, \n",
    "                                       fill=True, alpha=0.25, linewidth=1))\n",
    "        heatmap.add_patch(patches.Rectangle((current_state[1], 7-current_state[0]), 1, 1, \n",
    "                                       fill=False, color='white', linewidth=8))\n",
    "        \n",
    "    # save fig\n",
    "    heatmap.get_figure().savefig('png_for_gif/' + 'traversing_step_' + str(t) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_traversal_gif(t, image_path='png_for_gif/*.png', gif_path='png_for_gif/'):\n",
    "    \n",
    "    # grab images to GIFify\n",
    "    images = natsorted(glob.glob(image_path))\n",
    "    gif_name = gif_path + 'traversal_in_' + str(t) + '_steps.gif'\n",
    "\n",
    "    with imageio.get_writer(gif_name, mode='I',  duration=0.5) as writer:\n",
    "        for image_name in images:\n",
    "            image = imageio.imread(image_name)\n",
    "            writer.append_data(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traversing maze...\n",
      "Current state is: [1, 1]\n",
      "Current state is: [2, 1]\n",
      "Current state is: [3, 1]\n",
      "Current state is: [3, 2]\n",
      "Current state is: [3, 3]\n",
      "Current state is: [3, 4]\n",
      "Current state is: [3, 5]\n",
      "Current state is: [3, 6]\n",
      "Current state is: [4, 6]\n",
      "Current state is: [5, 6]\n"
     ]
    }
   ],
   "source": [
    "traverse_grid(q2, lava_grid=True, plot_each_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get this GIF of our path across the maze out as a result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Traversal](https://raw.githubusercontent.com/jagex-data-science/maze_runner/master/q2_traversal_in_10_steps.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is ok but we can probably do better. In particular, we aren't stopping to pick up the +20 reward.. maybe if we bias the agent towards preferring shorter term rewards, this could do the trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q3 = learn_maze(reward_matrix, lava_grid=True, alpha=0.8, gamma=0.9, epsilon=0.8, \n",
    "               n_episodes=100, t_per_episode=10000, verbose=False, plot_often=False, plot_episode_end=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA44AAAFlCAYAAACk67b8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11205d390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_q_values(q3, reward_matrix, plot_reward=True, save_q_plot=True, t=0, episode=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traversing maze...\n",
      "Current state is: [1, 1]\n",
      "Current state is: [2, 1]\n",
      "Current state is: [3, 1]\n",
      "Current state is: [3, 2]\n",
      "Current state is: [3, 3]\n",
      "Current state is: [3, 4]\n",
      "Current state is: [3, 5]\n",
      "Current state is: [3, 6]\n",
      "Current state is: [4, 6]\n",
      "Current state is: [5, 6]\n"
     ]
    }
   ],
   "source": [
    "traverse_grid(q3, lava_grid=True, plot_each_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Traversal](https://raw.githubusercontent.com/jagex-data-science/maze_runner/master/q3_traversal_in_10_steps.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal about the eventual mega future reward of +100 has been spread further back, which is nice to see. But... the agent is not really picking up on the minor rewards as well as you'd think. Maybe the agent needs a bit of a push to do more random exploration? More episodes? Maybe it'll do a better job with [SARSA updates rather than Q learning updates](https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html)? See if you can get an improved result (classic 'left as an exercise to the reader' escape). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "So, in short, using a very basic version of Q learning, we have a general method for getting solutions to grid worlds. \n",
    "\n",
    "Some ideas of experiments you might want to try (that might also be covered in the next blog post):\n",
    "    1. Effects of increasing number of learning episodes; observing and measuring convergence\n",
    "    2. Cutting episodes short by restricting max t\n",
    "    3. Changing keeness to update previous knowledge (gamma)\n",
    "    4. Changing the learning rate (alpha)\n",
    "    5. Changing the update rule to SARSA\n",
    "    6. Changing probability of random actions (epsilon)\n",
    "    7. Normalisation of Q values by state (e.g. to be between 0 and 1)\n",
    "    8. Playing around with different maze configurations\n",
    "\n",
    "It'd also be nice to have better Q value visualisations - instead of values being averaged in all directions to get one value per state, it would be better to represent action-specific utility values with little arrows with variable thickness (like in [Richard Sutton's grid world demo at 41:00](https://youtu.be/ggqnxyjaKe4?t=2463)). But.. this seemed like a pain to do in matlab. \n",
    "\n",
    "Also, if you've been running your own simulations, you might have noticed that there is some instability... sometimes the Q value estimates end up in such a state that when the agent greedily chooses the optimal action, it gets stuck. Doing epsilon-greedy action selection would get you out of the infinite loop, but there are probably better ways to promote stability.\n",
    "\n",
    "If you found this post particularly interesting or find some mistakes (confession: am noob), let me know :) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
