{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your own maze-solving AI agent in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning (RL) is a subfield of machine learning/AI in which an agent acts in an environment and learns to maximise the total reward it receives. Think a video game player agent learning to maximise its score in [Atari games](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/), or an autonamous [helicopter learning to do aerial manuevres for rewards](https://www.youtube.com/watch?v=VCdxqn0fcnE), or a humanoid robot learning how to best [adjust its limb positions and joint torques to stand up](https://gym.openai.com/envs/HumanoidStandup-v1)(sort of..). In this post, we want to make a reinforcement learning agent that can learn to escape a grid-like environment while maximising rewards. Specifically, there is a worm that needs to get out of a maze as quickly as possible:\n",
    "\n",
    "![Basic maze](basic_maze.png)\n",
    "\n",
    "This is a pretty tame maze (nothing really on it - just +100 reward for getting to the end, and -1 everywhere else), so let's also make an advanced maze surrounded with dragon-filled lava (-100 reward, not good), a couple of perfectly-spherical trolls (-20 reward) and a treat (+20 reward, maybe a sugar cube... what do worms eat again?).\n",
    "\n",
    "![Advanced maze](advanced_maze.png)\n",
    "\n",
    "We'll use basic Python (+numpy/pandas) to allow the worm to learn how 'good' each state and action is - this goodness is represented by the values in the heatmap below. In this post, you'll see that the major trick is that these values incorporate information about potential future rewards leading on from that state. This means that once the agent knows these values (called Q values - we'll discuss), it can then make greedy decisions that maximise this value at each time step. This lets us solve simple grid worlds of different varieties, and we can then plot our trajectory within the grid world/maze.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jagex-data-science/maze_runner/master/traversal_in_10_steps_random_p0.0.gif\")>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Traversal](https://raw.githubusercontent.com/jagex-data-science/maze_runner/master/traversal_in_10_steps_random_p0.0.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![random](https://media.giphy.com/media/XOF6spgWpYXpm/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want the code to play around with, check out [this notebook](https://github.com/jagex-data-science/maze_runner/blob/master/maze_post_to_proofread.ipynb). The rest of the post will go through what the code is doing. Once everything is in place, we'll do some experiments to see how changes to parameters (learning rate, exploration/exploitation trade-offs, number of episodes, episode length, on-policy/off-policy method) affect our learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The maze problem\n",
    "\n",
    "First let's introduce some language for talking about RL problems, and then get into the code.\n",
    "\n",
    "RL deals with sequential environments indexed by time points. We say that at any time point the worm can be in any number of ***states*** (the different squares on the grid) and can take various ***actions*** from each state (here, our action space is [left, right, up and down] - but in some spots, like corner states, not all actions are available). Each state is associated with a ***reward*** $r$ - this is just a number, and is a property of the environment. So, we can make decisions about which action $a$ to take in a given state $s$ at time point $t$, and the environment gives a reward $r$ and puts us in a ***successor state*** $s'$. Each episode ends when the worm gets to ***terminal state*** - the end of the maze. Then it is (cruelly) transported back to the beginning where it has to start again. Let's say there's no dying/respawning, so if you get in lava you're punished severely (-100 reward) but you can still crawl out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "Let's start setting up the environment. We need a reward matrix. For the simple maze, the numpy reward matrix looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # imports first\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "# %matplotlib inline\n",
    "# sns.set(font=\"monospace\")\n",
    "# random.seed(42)\n",
    "\n",
    "# # for making GIFs\n",
    "# import matplotlib.patches as patches\n",
    "# import glob\n",
    "# import imageio\n",
    "# from natsort import natsorted\n",
    "\n",
    "# basic maze state rewards\n",
    "reward_matrix = np.array([[-1, -1, -1, -1, -1, -1],\n",
    "                          [-1, -1, -1, -1, -1, -1],\n",
    "                          [-1, -1, -1, -1, -1, -1],\n",
    "                          [-1, -1, -1, -1, -1, -1],\n",
    "                          [-1, -1, -1, -1, -1, -1],\n",
    "                          [-1, -1, -1, -1, -1, 100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the complicated maze, we've added a few other rewards. The numpy version is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-100., -100., -100., -100., -100., -100., -100., -100.],\n",
       "       [-100.,   -1.,   -1.,   -1.,   -1.,   -1.,   -1., -100.],\n",
       "       [-100.,   -1.,  -20.,   -1.,   20.,   -1.,   -1., -100.],\n",
       "       [-100.,   -1.,   -1.,   -1.,   -1.,   -1.,   -1., -100.],\n",
       "       [-100.,   -1.,   -1.,   -1.,   -1.,  -20.,   -1., -100.],\n",
       "       [-100.,   -1.,   -1.,   -1.,   -1.,   -1.,   -1., -100.],\n",
       "       [-100.,   -1.,   -1.,   -1.,   -1.,   -1.,  100., -100.],\n",
       "       [-100., -100., -100., -100., -100., -100., -100., -100.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# advanced maze state rewards\n",
    "reward_matrix = np.array([[-1,  -1, -1,  -1,  -1,  -1],\n",
    "                          [-1, -20, -1,  20,  -1,  -1],\n",
    "                          [-1,  -1, -1,  -1,  -1,  -1],\n",
    "                          [-1,  -1, -1,  -1, -20,  -1],\n",
    "                          [-1,  -1, -1,  -1,  -1,  -1],\n",
    "                          [-1,  -1, -1,  -1,  -1, 100]]).astype(\"float32\")\n",
    "\n",
    "reward_matrix = np.pad(reward_matrix, pad_width=1, mode='constant', constant_values=-100)\n",
    "reward_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just stick to this more interesting matrix for now. \n",
    "\n",
    "The worm can go through the maze using four actions - up, down, left or right: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions = np.array(['up', 'down', 'left', 'right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's easy to see how these actions change your position in grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move(current_state, action):\n",
    "    \n",
    "    next_state_x = current_state[0]\n",
    "    next_state_y = current_state[1]\n",
    "    \n",
    "    if action=='up':\n",
    "        next_state_x -=1\n",
    "    elif action=='down':\n",
    "        next_state_x +=1\n",
    "    elif action=='left':\n",
    "        next_state_y -=1\n",
    "    elif action=='right':\n",
    "        next_state_y +=1\n",
    "        \n",
    "    return [next_state_x, next_state_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move([0,0], 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a little function to get the possible actions based on current position (e.g. in corner spots, a worm can only go in two directions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_possible_actions(current_state, possible_actions=actions, \n",
    "                         nrows=reward_matrix.shape[0], ncols=reward_matrix.shape[1]):\n",
    "\n",
    "    if current_state[1]==ncols-1:\n",
    "        possible_actions = np.setdiff1d(possible_actions, 'right')\n",
    "    elif current_state[1]==0:\n",
    "        possible_actions = np.setdiff1d(possible_actions, 'left')\n",
    "    if current_state[0]==0:\n",
    "        possible_actions = np.setdiff1d(possible_actions, 'up')\n",
    "    elif current_state[0]==nrows-1:\n",
    "        possible_actions = np.setdiff1d(possible_actions, 'down')\n",
    "    \n",
    "    return possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the possible actions in the lava corner [0,0] (take a look at the original grid) are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['down', 'right'], \n",
       "      dtype='|S5')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_possible_actions([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we can figure out how the worm can move on the grid now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some of the maths behind the problem\n",
    "\n",
    "This section isn't really crucial to the post so feel free to skip it, but I recently learned Latex so I've got an itch to type equations. \n",
    "\n",
    "As the worm goes through the maze, it will acquire a string of rewards. It will get some reward for time step $t$, $t+1$, $t+2$ and so on. The total reward in an episode (the return $G_t$ from that time point onwards) is just the sum:\n",
    "\n",
    "\\begin{align*}\n",
    "    G_t = R_{t+1} + R_{t+2} + R_{t+3} \\ ... \\ + R_{n} = \\sum_{t}^n R_{t+1}\n",
    "\\end{align*}\n",
    "\n",
    "We want to encode some preference for immediate rewards, with future rewards being worth less - more uncertain, preference towards recency. Enter gamma discounting. The parameter $\\gamma$ controls our preference for immediate versus long-term rewards. \n",
    "\n",
    "\\begin{align*}\n",
    "    G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\ ... \\ = \\sum_{k=0}^{\\inf} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "\n",
    "If $\\gamma$==0, then we are only interested in the most immediate rewards (all rewards after time $t$ are multiplied by 0 and disappear. If $\\gamma$==1, then we are just as interested in far away future rewards as the most immediate reward. And gamma can be any value in between.  So, we've introduced a judgement where we prefer immediate reward (unless $\\gamma$ is 1). Say we have a discount factor of gamma=0.5, we half the amount we care about returns at each step. Notice that the very next reward $R_{t+1}$ isn't discounted by $\\gamma$.\n",
    "\n",
    "The value of a state is actually defined as the expectation of the return when you start in that state. \n",
    "\n",
    "\\begin{align*}\n",
    "    v(s) = \\mathbb{E}[G_t \\ | \\ S_t=s]\n",
    "\\end{align*}\n",
    "\n",
    "Let's substitute the definition of $G_t$ back in. \n",
    "\n",
    "\\begin{align*}\n",
    "    v(s) =& \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 R_{t+3} + ... \\ | \\ S_t = s]\n",
    "\\end{align*}\n",
    "   \n",
    "    \n",
    "You might have noticed that the equation for return $G_t$ has a bit of a recursive structure. It's made up of two parts - the immediate reward $R_{t+1}$ and the discounted rewards for $t+2$ onwards (i.e. $\\gamma R_{t+2} + \\gamma ^2 R_{t+3} + ... $). But, this second part is just the $\\gamma$ times the rewards for the next time step onwards!\n",
    "\n",
    "\\begin{align*}\n",
    "    v(s) =& \\ \\mathbb{E} [G_t \\ | \\ S_t = s] \\\\\n",
    "         =& \\  \\mathbb{E} [R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 R_{t+3} + ... \\ | \\ S_t = s] \\\\\n",
    "         =& \\ \\mathbb{E} [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + ... ) \\ | \\ S_t = s] \\\\\n",
    "         =& \\ \\mathbb{E} [R_{t+1} + \\gamma (G_{t+1}) \\ | \\ S_t = s] \n",
    "\\end{align*}\n",
    "\n",
    "In the expanded definition of $G_t$, the $\\gamma$ discounting factor can be factorised out. At this point, you can see that $\\gamma (R_{t+2} + \\gamma R_{t+3} + ... $ is simply $\\gamma$ times the value function of the next state $t+1$. \n",
    "In other words, the value function can be decomposed into two parts. (This is a version of the [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation)). \n",
    "\n",
    "The point of this section is to emphasise that the value of a state depends on the rewards you expect to see in future states after it. We'll use this idea to solve the maze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should the worm solve the maze?\n",
    "\n",
    "Intuitively, for each state we find ourselves in, we want to choose the action that maximises future cumulative reward. If at every point, we knew what the best possible action was (the one that is going to maximise future cumulative reward), then we've solved the problem. We would just go ahead and take the best action at all times. Because the biggest reward is at the end of the maze, these 'best actions' should gradually take us closer to our ultimate goal of escape.\n",
    "\n",
    "This means that the problem the worm actually has to solve is estimating how good it is to take different actions in each state. It has no prior knowledge or model of the environment - to learn these estimates, it'll just have to explore around and gradually build up information.\n",
    "\n",
    "Formally, these values are ***utility*** values and are called Q values. For each state $s$ and action $a$ available in that state, there is a corresponding $Q(s,a)$ we want to learn. These utility values tells us how good an action is in a given state. It's defined as the immediate reward you get for making an action (this is the $r(s,a)$ part), plus the best/max utility you can get from the successor state $s'$:\n",
    "\n",
    "\\begin{align*}\n",
    "    Q(s,a) =& \\ r(s,a) + \\gamma \\max_{a'} Q(s', a')\n",
    "\\end{align*}\n",
    "\n",
    "Initially, when we enter the maze for the first time, we have no idea what these Q values are. We could initialise all values with 0 or with some random small number (e.g. between 0 and 0.1). This is a bit abstract... looking at a concrete example helps. The below function will generate a starting Q table (pass `use_zero=True` to initialise with zeros):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialise_q_table(reward_matrix, use_zero=False):\n",
    "    \n",
    "    nrows=reward_matrix.shape[0]\n",
    "    ncols=reward_matrix.shape[1]\n",
    "    \n",
    "    # get possible states and actions\n",
    "    states = pd.DataFrame([[[x, y]] for x in range(nrows) for y in range(ncols)], columns=['state'])\n",
    "    states['str_state'] = [str(state) for state in states['state']]\n",
    "    states['actions'] = [get_possible_actions(state) for state in states['state']]\n",
    "\n",
    "    # clean up actions, one row per possible action\n",
    "    clean_actions = pd.DataFrame(states['actions'].apply(pd.Series, 1).stack(), columns=['action'])\n",
    "    clean_actions.index = clean_actions.index.droplevel(-1)\n",
    "    states = states.join([clean_actions]).drop('actions', axis=1)\n",
    "    states.head()\n",
    "\n",
    "    # initiate values to be 0 or random small numbers\n",
    "    if use_zero:\n",
    "        states['value'] = 0\n",
    "    else:\n",
    "        states['value'] = np.random.uniform(-0.1,0.1,states.shape[0])\n",
    "    \n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to see what this looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>str_state</th>\n",
       "      <th>action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.088719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.053906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>down</td>\n",
       "      <td>-0.099292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.094281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.045629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.077985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.001175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.081815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.024194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.094344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state str_state action     value\n",
       "0  [0, 0]    [0, 0]   down  0.088719\n",
       "0  [0, 0]    [0, 0]  right -0.053906\n",
       "1  [0, 1]    [0, 1]   down -0.099292\n",
       "1  [0, 1]    [0, 1]   left -0.094281\n",
       "1  [0, 1]    [0, 1]  right  0.045629\n",
       "2  [0, 2]    [0, 2]   down  0.077985\n",
       "2  [0, 2]    [0, 2]   left -0.001175\n",
       "2  [0, 2]    [0, 2]  right  0.081815\n",
       "3  [0, 3]    [0, 3]   down  0.024194\n",
       "3  [0, 3]    [0, 3]   left  0.094344"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = initialise_q_table(reward_matrix, use_zero=False)\n",
    "q.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are storing our estimates of expected rewards in a table, with one value for each state we can be in and each action we can take in that state. For example, the current (randomly generated) Q value associated with start in position [0,0] on the grid (in lava, actually) and taking action 'down' is 0.089, and going right is -0.05. These are random numbers, but if these were actually learned, then this would be useful information when deciding whether to go down or right from [0,0] - probably go down. \n",
    "\n",
    "Just to recap where we are so far, let's draw our reward grid and this gibberish grid of randomised Q values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_q_values(nrows=reward_matrix.shape[0], ncols = reward_matrix.shape[1], \n",
    "                  plot_reward=False, save_q_plot=True, t=0, episode=0):\n",
    "    \n",
    "    avg_q_value_by_state = q.groupby('str_state')['value'].mean().values.reshape(nrows,ncols)\n",
    "\n",
    "    if plot_reward==True:\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "        sns.heatmap(reward_matrix, ax=ax1)\n",
    "        sns.heatmap(avg_q_value_by_state, ax=ax2)\n",
    "        fig.set_size_inches(16, 6)\n",
    "    else: \n",
    "        plt.figure(figsize=(7,6))\n",
    "        heatmap = sns.heatmap(avg_q_value_by_state)        \n",
    "        if save_q_plot:\n",
    "            heatmap.get_figure().savefig('/Users/natasha_latysheva/Projects/jagex_data_science_blog/avg_q_value_episode_' \\\n",
    "                                         + str(episode) + '_time_' + str(t) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5QAAAFlCAYAAACUZ6fdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111323750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_q_values(nrows=reward_matrix.shape[0], ncols = reward_matrix.shape[1], \n",
    "              plot_reward=True, save_q_plot=True, t=0, episode=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left hand plot is the reward grid, which looks like what we'd expect. And yup, the right hand plot (Q values) looks pretty random. A little bit [Mondrian](https://www.google.co.uk/search?q=mondrian+paintings&source=lnms&tbm=isch&sa=X&ved=0ahUKEwjP2dPs4NvUAhWDPBQKHXSSAM4Q_AUICigB&biw=2100&bih=1127)?\n",
    "\n",
    "Anyway, how do we move beyond random Q values? How do we learn useful Q values that can help us navigate a maze? For that, we'll need to do some ***Q learning*** (of [DeepMind Atari fame](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)). There's actually many different varieties of Q-learning, and here we have a special simple case where there are very few, discrete states and actions (this is a tabular method), we aren't doing any feature extraction from the environment (e.g. no [deep convnets for Q learning](https://vmayoral.github.io/robots,/ai,/deep/learning,/rl,/reinforcement/learning/2016/08/07/deep-convolutional-q-learning/)), and we aren't keeping any sort of running memory (e.g. [eligiblity traces](https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q function update rule\n",
    "\n",
    "We will learn the real Q values iteratively using a simple update rule. Basically, to update $Q(s, a)$ at time $t$ (so, $Q(s_t, a_t)$), we will do a one step look ahead and find the maximum Q value you could immediately get from that successor state. In other words we peak ahead one step and find the estimate of the optimal future state. This is $\\max_{a} Q(s_{t+1}, a_t)$. We take our previous Q value $Q(s_t, a_t)$ for a state-action pair at time t, and make the following update:\n",
    "\n",
    "\\begin{align*}\n",
    "    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha * (r_{t+1} + \\gamma*  max_a Q(s_{t+1}, a_t) -  Q(s_t, a_t))   \n",
    "\\end{align*}\n",
    "\n",
    "The alpha parameter $\\alpha$ controls your learning rate (higher values weight new information higher, overwrites old information more). The gamma parameter $\\gamma$ (the 'discount factor') controls how much we prefer recent rewards to far-away rewards - if it's 0, we only ever care about immediate rewards, and if it's 1, we care about very distant rewards exactly as much as recent ones. You can change up alpha and gamma in clever ways as the algorithm trains but we'll just set them to... 0.8.\n",
    "\n",
    "Actually it's interesting to note that if you set alpha to 1.0 then the update rule becomes very short:\n",
    "\n",
    "\\begin{align*}\n",
    "    Q(s_t, a_t) = r_{t+1} + \\gamma*  max_a Q(s_{t+1}, a_t)    \n",
    "\\end{align*}\n",
    "\n",
    "This is a recursive definition, since the current Q value depends on the immediate reward plus the discounted maximum future Q value. In this way, the $Q(s,a)$ estimates take into account the Q values of future states. The values will gradually converge after many iterations (often surprisingly few iterations). Information about future rewards eventually trickles back to the value estimates of states at the beginning of the maze - this will become clear through the code examples.\n",
    "\n",
    "The code for doing one Q update is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_q_value(current_state, next_state, action, alpha, gamma, terminal_state, verbose=False):\n",
    "    \n",
    "    # make move and collect reward\n",
    "    reward = reward_matrix[next_state[0], next_state[1]]\n",
    "    \n",
    "    # the current q for the state + action we just took\n",
    "    current_q = q.loc[((q['str_state']==str(current_state)) & (q['action']==action)), 'value'].values[0]\n",
    "        \n",
    "    # get maximum q value leading on from next_state\n",
    "    max_future_q_value = np.max(q[q['str_state']==str(next_state)]['value'])\n",
    "    \n",
    "    # q value update rule\n",
    "    new_q = current_q + alpha*(reward + gamma*max_future_q_value - current_q)\n",
    "     \n",
    "    # update relevant value in q table\n",
    "    q.loc[((q['str_state']==str(current_state)) & (q['action']==action)), 'value'] = new_q\n",
    "    \n",
    "    # if next state is the terminal state, update that q value to reward\n",
    "    if str(next_state) == str(terminal_state):\n",
    "        q.loc[(q['str_state']==str(next_state)), 'value'] = 100\n",
    "\n",
    "    if verbose:\n",
    "        print 'Q of state {0} taking action \"{1}\" was updated from {2} to {3}'.format(current_state, action, current_q, new_q)\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to do a one step update. Remember this is our q table so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>str_state</th>\n",
       "      <th>action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.088719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>right</td>\n",
       "      <td>-0.053906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>down</td>\n",
       "      <td>-0.099292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.094281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.045629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.077985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.001175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.081815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.024194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.094344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state str_state action     value\n",
       "0  [0, 0]    [0, 0]   down  0.088719\n",
       "0  [0, 0]    [0, 0]  right -0.053906\n",
       "1  [0, 1]    [0, 1]   down -0.099292\n",
       "1  [0, 1]    [0, 1]   left -0.094281\n",
       "1  [0, 1]    [0, 1]  right  0.045629\n",
       "2  [0, 2]    [0, 2]   down  0.077985\n",
       "2  [0, 2]    [0, 2]   left -0.001175\n",
       "2  [0, 2]    [0, 2]  right  0.081815\n",
       "3  [0, 3]    [0, 3]   down  0.024194\n",
       "3  [0, 3]    [0, 3]   left  0.094344"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would the q table look like after we do one step to the right, to cell [0,1]? This is actually a lava cell on the grid, so we hope that this would be reflected in the updated value.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q of state [0, 0] taking action \"right\" was updated from -0.0539056131055 to -79.9815785052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-100.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_q_value(current_state=[0,0], next_state=[0,1], action='right', \n",
    "               alpha=0.8, gamma=0.8, terminal_state=[6,6], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new value replaces the previous `Q(s=[0,0], a='right')` entry in the table. \n",
    "\n",
    "We can double check this by hand. Recall that the reward is -100 for being in lava, `alpha` and `gamma` are both 0.8, and the previous estimate of $Q(s,a)$ was -0.0539056131055. \n",
    "\n",
    "The only tricky part is that we need to find the highest Q value associated with the successor state. Our starting state was [0,0] and the action we chose (maybe randomly) was 'right', so this means that the successor state is [0,1]. Looks like there are three actions we can take at state [0,1]: down, left, right. Of these, going right has the highest Q value (which is 0.045629). Plugging these values in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-79.98157856262111"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_q = -0.0539056131055 + 0.8*(-100 + 0.8*0.045629 - -0.0539056131055)\n",
    "new_q   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! What if we then went back left? Q table looks like this now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>str_state</th>\n",
       "      <th>action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.088719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>right</td>\n",
       "      <td>-79.981579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>down</td>\n",
       "      <td>-0.099292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.094281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.045629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.077985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>left</td>\n",
       "      <td>-0.001175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.081815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.024194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.094344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state str_state action      value\n",
       "0  [0, 0]    [0, 0]   down   0.088719\n",
       "0  [0, 0]    [0, 0]  right -79.981579\n",
       "1  [0, 1]    [0, 1]   down  -0.099292\n",
       "1  [0, 1]    [0, 1]   left  -0.094281\n",
       "1  [0, 1]    [0, 1]  right   0.045629\n",
       "2  [0, 2]    [0, 2]   down   0.077985\n",
       "2  [0, 2]    [0, 2]   left  -0.001175\n",
       "2  [0, 2]    [0, 2]  right   0.081815\n",
       "3  [0, 3]    [0, 3]   down   0.024194\n",
       "3  [0, 3]    [0, 3]   left   0.094344"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd expect this change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-79.96207604"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_q = -0.094281 + 0.8*(-100 + 0.8*0.088719 - -0.094281)\n",
    "new_q   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which our function should be able to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q of state [0, 1] taking action \"left\" was updated from -0.0942807842647 to -79.9620758104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-100.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_q_value(current_state=[0,1], next_state=[0,0], action='left', \n",
    "               alpha=0.8, gamma=0.8, terminal_state=[6,6], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen to the Q value next to the terminal state, the escape corner at [6,6]? Say we start in state [6, 5] and take a right. Let's pull out the relevant rows of Q values from the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>str_state</th>\n",
       "      <th>action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>up</td>\n",
       "      <td>-0.029473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>down</td>\n",
       "      <td>0.007947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>left</td>\n",
       "      <td>0.063252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>[6, 5]</td>\n",
       "      <td>right</td>\n",
       "      <td>0.037774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state str_state action     value\n",
       "53  [6, 5]    [6, 5]     up -0.029473\n",
       "53  [6, 5]    [6, 5]   down  0.007947\n",
       "53  [6, 5]    [6, 5]   left  0.063252\n",
       "53  [6, 5]    [6, 5]  right  0.037774"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[q['str_state']=='[6, 5]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q of state [6, 5] taking action \"right\" was updated from 0.0377742980307 to 80.0664819946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_q_value(current_state=[6,5], next_state=[6,6], action='right', \n",
    "               alpha=0.8, gamma=0.8, terminal_state=[6,6], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The fact that there is a big reward at the end of taking the action 'right' from state [6,5] gets reflected in a higher Q(s=[6,5], a='right') value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning all the Q values iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above little tidbits showed the effect of single updates in isolation, but of course we want to do many, many updates during out training episode, until the point that the Q value estimates are stable. Once stability/convergence is reached, the hope is that the new table will accurately represent how good it is to be in any state and take any action in our environment.\n",
    "\n",
    "Just one more thing to think about before we run the training. It's possible to learn good Q values by randomly walking around the maze (taking actions completely at random) and just observing which rewards come your way. But another approach is to take actions that you think are good, at the same time as you're building your estimates of how good they are. This latter approach is called an ***on-policy*** method - the policy is just telling us how to behave, and the approach is on-policy because our actions are influenced by the Q values we learn about. The random approach is ***off-policy***, because even though we are building Q value estimates of how to behave, we aren't following that advice, and instead are using a totally different, random policy to move around the world. \n",
    "\n",
    "So basically, we can learn about the optimal policy while being maximally exploratory. Alternatively, as we gather information, we can let this immediately change how we behave. \n",
    "\n",
    "The parameter epsilon ($\\epsilon$) controls to what extent our decisions about actions will be random. Do we *always* want to make random decisions about movement (off-policy; set $\\epsilon$>1), or only sometimes make random decisions (0<$\\epsilon$<1), or whether we'll always go where the utility values look best ($\\epsilon$=0). Let's try `epsilon=0.5`, which will lead to random movements 50% of the time, but 50% of the time we choose the best-looking action.\n",
    "\n",
    "The function to run the entire simulation is this (explanation below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_maze(reward_matrix, lava_grid=False, alpha=0.8, gamma=0.8, epsilon=0.2, \n",
    "               n_episodes=15, t_per_episode=1000, verbose=False, \n",
    "               plot_often=False, plot_episode_end=True):\n",
    "\n",
    "    # initialise q table\n",
    "    q = initialise_q_table(reward_matrix, use_zero=False)\n",
    "\n",
    "    # episodes end when terminal state reached (==escaped maze)\n",
    "    for episode in range(int(n_episodes)):\n",
    "\n",
    "        print 'Commencing episode number {0}...'.format(episode)\n",
    "        t = 0\n",
    "        escaped = False\n",
    "        \n",
    "        # begin in the top left corner of the grid (but not in the lava!)\n",
    "        if lava_grid:\n",
    "            current_state = [1,1]\n",
    "            terminal_state = [6,6]        \n",
    "        else:\n",
    "            current_state = [0,0]\n",
    "            terminal_state = [5,5]\n",
    "\n",
    "        # while you're in the maze, make decisions about actions\n",
    "        while not escaped and t < t_per_episode:\n",
    "\n",
    "            # get q table info relating to your current state\n",
    "            current_state_slice = q[q['str_state'] == str(current_state)]\n",
    "\n",
    "            # small chance of acting randomly (epsilon)\n",
    "            if np.random.rand() < epsilon:\n",
    "                valid_moves = np.array(current_state_slice['action'])\n",
    "                action = np.random.choice(valid_moves)\n",
    "\n",
    "            # otherwise, do what looks best\n",
    "            else:\n",
    "                'Taking best action'\n",
    "                best_action_index = np.argmax(np.array(current_state_slice['value']))\n",
    "                action = np.array(current_state_slice['action'])[best_action_index]           \n",
    "\n",
    "            next_state = move(current_state, action)\n",
    "\n",
    "            # get your reward and update your q value\n",
    "            update_q_value(current_state, next_state, action, alpha=alpha, gamma=gamma, \n",
    "                           terminal_state=terminal_state, verbose=verbose)\n",
    "\n",
    "            # if you reached the terminal state, you win gg\n",
    "            if str(next_state) == str(terminal_state):\n",
    "                print 'Escaped maze in {0} time steps.'.format(t)\n",
    "                escaped = True\n",
    "                \n",
    "                if plot_episode_end:\n",
    "                    plot_q_values(save_q_plot=True, t=t, episode=episode)\n",
    "\n",
    "            # update current state and time step\n",
    "            # occasionally plot q values\n",
    "            if plot_often:\n",
    "                if t == 0 or t % 500 == 0:\n",
    "                    print 'Current state at time {0} is: {1}'.format(t, current_state)\n",
    "                    plot_q_values(save_q_plot=True, t=t, episode=episode)\n",
    "\n",
    "            t += 1\n",
    "            current_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is a bit dense-looking but is actually composed of very simple things:\n",
    "\n",
    "+ **Line 6**: Initialise the Q values randomly (or to all zeros)\n",
    "+ **Line 9**: We'll let the worm do many different runs through the maze\n",
    "+ **Lines 16-21**: Depending on whether you want to run the basic maze or the advanced maze with the lava moat, the starting position and the position (the terminal state) will vary. \n",
    "+ **Line 24**: Given that you haven't escaped (reached the terminal state), and your time hasn't run out (arbitrary n), make decisions about where to go next in the grid.\n",
    "+ **Line 27**: Just taking a slice of the Q table that relates to our current state, for convenience, since we'll be using these values below.\n",
    "+ **Lines 30 to 32**: Sample a random value between 0 and 1 using np.random.rand(). If this is less than epsilon, choose a random action out of the available ones\n",
    "+ **Lines 35 to 38**: If the random value wasn't less than epsilon, choose the action associated with the highest utility. \n",
    "+ **Line 40**: whichever action you chose, now's the time to get the next state it leads to.\n",
    "+ **Line 43**: Update your Q value by looking ahead one step from this `next_state` (see previous sections for explanation). \n",
    "+ **Lines 47 to 52**: Check if this next state is actually the terminal state. If it is, you reached the end of the maze! Make a self-congratulatory plot of the q table as it stands (if `plot_episode_end==True`) and set `escaped` to `True`.\n",
    "+ **Lines 56 to 59**: A bit of monitoring code - if you'd like to keep a closer tab on the Q values as they evolve, pass `plot_often=True`. \n",
    "+ **Lines 61 and 62**: Incrementing the time step and updating our current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run one episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing episode number 0...\n",
      "Escaped maze in 45 time steps.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAFlCAYAAADMCx4cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1121be950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_maze(reward_matrix, lava_grid=False, alpha=0.95, gamma=0.8, epsilon=0.5, \n",
    "               n_episodes=1, t_per_episode=1000, verbose=False, plot_often=False, plot_episode_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see that reward grid again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4gAAAFlCAYAAACp9ca7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0GJ0QQEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112097210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_q_values(nrows=reward_matrix.shape[0], ncols = reward_matrix.shape[1], \n",
    "              plot_reward=True, save_q_plot=True, t=0, episode=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, definitely looks inspired by the reward grid, even after just 1 short episode. Given that we were plonked into this world with absolutely zero knowledge about the world (we could only try things out and get feedback from the environment), we now already know quite a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Q values to quickly leave the maze\n",
    "\n",
    "Now that we've gathered lots of information about the Q values (though normally we'd train for much, much longer), traversing the maze again is ez mode. For each state we're in, we are just going to greedily choose the available action with the highest utility, and keep doing that until we escape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traverse_grid(q, lava_grid=False, random_action_prob=0.0, plot_each_step=True):\n",
    "    \n",
    "    print 'Traversing maze...'\n",
    "    \n",
    "    if lava_grid:\n",
    "        current_state = [1, 1]\n",
    "        terminal_state = [6, 6]\n",
    "    else:\n",
    "        current_state = [0, 0]\n",
    "        terminal_state = [5, 5]\n",
    "    \n",
    "    t=0\n",
    "    \n",
    "    # traverse greedily\n",
    "    while str(current_state) != str(terminal_state):\n",
    "\n",
    "        print 'Current state is: {0}'.format(current_state)\n",
    "        \n",
    "        if plot_each_step:\n",
    "            plot_current_position(current_state, t)\n",
    "\n",
    "        # lookup q values of actions in current states\n",
    "        current_state_slice = q[q['str_state'] == str(current_state)]\n",
    "        best_action_index = np.argmax(np.array(current_state_slice['value']))\n",
    "        action = np.array(current_state_slice['action'])[best_action_index] \n",
    "        \n",
    "        # make a move\n",
    "        next_state = move(current_state, action)\n",
    "\n",
    "        # if you won then plot it\n",
    "        if next_state == terminal_state and plot_each_step==True:\n",
    "            plot_current_position(next_state, t+1, reached_terminal=True)\n",
    "            \n",
    "        current_state = next_state\n",
    "        t += 1\n",
    "    \n",
    "    # when done, make gif of path taken\n",
    "    make_traversal_gif(t, random_action_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for traversing the grid is simple (extremely simlar to `learn_maze` code above) but plotting is a bit of a pain. We will need two helper functions to make nice GIFs to visualise our path in the maze. The `plot_current_position` function takes your current position and plots it on the learned Q values heatmap. The `make_traversal_gif` function uses [imageio](https://imageio.readthedocs.io/en/latest/) magic to spit out a gif (make sure the `image_path` only has .png files that you want included in your GIF... clean out those cat memes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_current_position(current_state, t, reward_matrix=reward_matrix, q=q, reached_terminal=False):\n",
    "    \n",
    "    # plot heatmap of q values\n",
    "    nrows=reward_matrix.shape[0]\n",
    "    ncols=reward_matrix.shape[1]\n",
    "    avg_q_value_by_state = q.groupby('str_state')['value'].mean().values.reshape(nrows,ncols)\n",
    "    plt.figure(figsize=(7,6))\n",
    "    heatmap = sns.heatmap(avg_q_value_by_state)        \n",
    "    \n",
    "    # add rectangle for current position\n",
    "    if not reached_terminal:\n",
    "        heatmap.add_patch(patches.Rectangle((current_state[0]-0.00, 7-current_state[1]-0.00), 1, 1, \n",
    "                                       fill=True, alpha=0.25, linewidth=1))\n",
    "        heatmap.add_patch(patches.Rectangle((current_state[0]-0.00, 7-current_state[1]-0.00), 1, 1, \n",
    "                                       fill=False, color='black', linewidth=5))\n",
    "    else: \n",
    "        heatmap.add_patch(patches.Rectangle((current_state[0]-0.00, 7-current_state[1]-0.00), 1, 1, \n",
    "                                       fill=True, alpha=0.25, linewidth=1))\n",
    "        heatmap.add_patch(patches.Rectangle((current_state[0]-0.00, 7-current_state[1]-0.00), 1, 1, \n",
    "                                       fill=False, color='white', linewidth=8))\n",
    "        \n",
    "    # save fig\n",
    "    heatmap.get_figure().savefig('png_for_gif/' + 'traversing_step_' + str(t) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_traversal_gif(t, random_action_prob, image_path='png_for_gif/*.png', gif_path='png_for_gif/'):\n",
    "    \n",
    "    # grab images to GIFify\n",
    "    images = natsorted(glob.glob(image_path))\n",
    "    gif_name = gif_path + 'traversal_in_' + str(t) + '_steps_random_p' + str(random_action_prob) + '.gif'\n",
    "\n",
    "    with imageio.get_writer(gif_name, mode='I',  duration=0.5) as writer:\n",
    "        for image_name in images:\n",
    "            image = imageio.imread(image_name)\n",
    "            writer.append_data(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traversing maze...\n",
      "Current state is: [1, 1]\n",
      "Current state is: [1, 2]\n",
      "Current state is: [1, 3]\n",
      "Current state is: [2, 3]\n",
      "Current state is: [2, 4]\n",
      "Current state is: [2, 5]\n",
      "Current state is: [3, 5]\n",
      "Current state is: [3, 6]\n",
      "Current state is: [4, 6]\n",
      "Current state is: [5, 6]\n"
     ]
    }
   ],
   "source": [
    "traverse_grid(q, lava_grid=True, plot_each_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get this GIF of our path across the maze out as a result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jagex-data-science/maze_runner/master/traversal_in_10_steps_random_p0.0.gif\")>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further work [for me]\n",
    "\n",
    "1. show results of trianing for much longer \n",
    "2. getting agent to pick up treats/avoid trolls (probably jsut need to train longer)\n",
    "3. Write total reward and time steps on heatmap plots per time step t\n",
    "4. Embed GIFs properly\n",
    "5. Do the experiments below? Or maybe this is for another post\n",
    "6. **** Put physically barriers/walls into maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of this code in place it's time to enjoy ourselves by running experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Many episodes versus one episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cutting episodes short by restricting max $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Changing keeness to update knowledge, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Changing the learning rate, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Off-policy completely random walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. On-policy learning with low epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. On-policy learning with high epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further work [for readers]\n",
    "\n",
    "A lot of improvements could be made to this post but this little project has already opened a time wormhole so I'll cut it there. Some suggestions for taking this further:\n",
    "    + Better Q value plots - instead of averaged by all directions to get one value per state, represent action-specific utility values with little arrows with variable thickness (like in Richard Sutton's grid world demo at 12:00)\n",
    "    + See the effects of SARSA update rule instead of Q learning update rule on learning\n",
    "    + Play around with different starting reward grids/maze configuations\n",
    "    + Normalisation of Q values by state\n",
    "\n",
    "If you found this post interesting or find some mistakes (confession: am noob), let me know in the comments below :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
